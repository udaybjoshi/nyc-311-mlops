# üóΩ NYC 311 Service Request Intelligence Platform - PoC

A **cost-optimized, production-grade data pipeline** for NYC 311 service request analysis using Databricks, AWS, and the Medallion Architecture. Built to stay **under $100** while demonstrating enterprise ML engineering practices.

---

## üìä Project Overview

This platform ingests NYC 311 service request data, processes it through Bronze ‚Üí Silver ‚Üí Gold layers, applies ML-based forecasting and anomaly detection, and delivers insights via an interactive dashboard.

### üéØ Business Value
- **Proactive Operations**: Detect anomalous spikes in service requests before they become critical
- **Resource Optimization**: Forecast demand patterns for better workforce allocation
- **Cost Efficiency**: 95% cost reduction vs. traditional 24/7 cluster approaches
- **Transparency**: Full ML lineage, auditability, and explainable insights

---

## üí∞ Cost Optimization Strategy (Target: <$100)

| Component | Strategy | Est. Cost |
|-----------|----------|-----------|
| **Databricks Compute** | Job clusters (auto-terminate) + Community Edition for dev | $30-40 |
| **AWS S3 Storage** | Lifecycle policies, compressed Delta tables | $5-10 |
| **Data Volume** | Sample last 90 days only (vs. 10+ years full dataset) | $0 |
| **API Calls** | Cached responses, incremental loads | $0 |
| **Development** | Local Spark for testing, CI/CD optimizations | $5-10 |
| **Monitoring** | Built-in Databricks metrics (no external tools) | $0 |
| **Total Estimate** | | **$40-60** |

### üîß Cost Control Measures
- ‚úÖ **Job Clusters**: Spin up only when needed, terminate after 5 min idle
- ‚úÖ **Spot Instances**: 70% discount on AWS spot for batch workloads
- ‚úÖ **Data Sampling**: Process 90-day rolling window (not full 10+ year dataset)
- ‚úÖ **Incremental Processing**: Delta Lake change data capture
- ‚úÖ **Auto-scaling**: Min 1 worker, max 3 workers
- ‚úÖ **Scheduled Jobs**: Run daily during off-peak hours (3 AM ET)
- ‚úÖ **Local Development**: Unit tests run on local Spark (no cluster costs)

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NYC Open Data  ‚îÇ
‚îÇ   311 API       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Daily Ingestion (Scheduled Job)
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         BRONZE LAYER (Raw Data)          ‚îÇ
‚îÇ  ‚Ä¢ Raw JSON from API                     ‚îÇ
‚îÇ  ‚Ä¢ Append-only Delta tables              ‚îÇ
‚îÇ  ‚Ä¢ Partition by ingestion_date           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Data Quality + Deduplication
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      SILVER LAYER (Cleaned Data)         ‚îÇ
‚îÇ  ‚Ä¢ Standardized schema                   ‚îÇ
‚îÇ  ‚Ä¢ Type casting + validation             ‚îÇ
‚îÇ  ‚Ä¢ Business rules applied                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Aggregation + Feature Engineering
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    GOLD LAYER (Analytics-Ready Data)     ‚îÇ
‚îÇ  ‚Ä¢ Daily aggregates by complaint_type    ‚îÇ
‚îÇ  ‚Ä¢ Time series features                  ‚îÇ
‚îÇ  ‚Ä¢ Ready for ML + BI tools               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                 ‚ñº                ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Prophet ‚îÇ    ‚îÇ   Anomaly    ‚îÇ  ‚îÇStreamlit ‚îÇ
    ‚îÇForecast ‚îÇ    ‚îÇ  Detection   ‚îÇ  ‚îÇDashboard ‚îÇ
    ‚îÇ Model   ‚îÇ    ‚îÇ  (Threshold) ‚îÇ  ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                 ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ    MLflow    ‚îÇ
          ‚îÇ  Experiment  ‚îÇ
          ‚îÇ   Tracking   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Project Structure

```
nyc-311-platform/
‚îÇ
‚îú‚îÄ‚îÄ .databricks/                 # Databricks configuration
‚îÇ   ‚îî‚îÄ‚îÄ bundle.yml              # Databricks Asset Bundle config
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml              # CI/CD pipeline
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml          # Deployment automation
‚îÇ
‚îú‚îÄ‚îÄ conf/
‚îÇ   ‚îú‚îÄ‚îÄ dev.yaml                # Development environment config
‚îÇ   ‚îú‚îÄ‚îÄ prod.yaml               # Production environment config
‚îÇ   ‚îî‚îÄ‚îÄ cluster_config.yaml     # Cost-optimized cluster specs
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 00_setup_and_exploration.py
‚îÇ   ‚îú‚îÄ‚îÄ 01_bronze_ingestion.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_silver_transformation.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_gold_aggregation.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_ml_forecasting.py
‚îÇ   ‚îú‚îÄ‚îÄ 05_anomaly_detection.py
‚îÇ   ‚îî‚îÄ‚îÄ 99_monitoring_dashboard.py
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ nyc311/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformations/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_to_silver.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ silver_to_gold.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forecasting.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anomaly_detection.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ delta_helpers.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ setup.py
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_transformations.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_ml_models.py
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îÇ       ‚îî‚îÄ‚îÄ test_pipeline_e2e.py
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py        # Dashboard application
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # AWS S3 + IAM setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îú‚îÄ‚îÄ setup_workspace.sh
‚îÇ       ‚îî‚îÄ‚îÄ cost_monitor.sh
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ cost_analysis.md
‚îÇ   ‚îî‚îÄ‚îÄ runbook.md
‚îÇ
‚îú‚îÄ‚îÄ .env.example                # Environment variables template
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ databricks.yml              # Databricks CLI config
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt        # Development dependencies
‚îú‚îÄ‚îÄ pytest.ini
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start (Under $100 Budget)

### Prerequisites
- AWS Account (free tier eligible)
- Databricks Account (Community Edition or trial)
- Python 3.9+
- Databricks CLI

### 1Ô∏è‚É£ Initial Setup (5 minutes, $0)

```bash
# Clone repository
git clone <your-repo-url>
cd nyc-311-platform

# Install dependencies locally (for development)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements-dev.txt

# Configure environment
cp .env.example .env
# Edit .env with your credentials
```

### 2Ô∏è‚É£ Infrastructure Setup (10 minutes, ~$5)

```bash
# Setup AWS resources (S3 buckets with lifecycle policies)
cd infrastructure/terraform
terraform init
terraform plan
terraform apply -var="environment=dev"

# Configure Databricks workspace
databricks configure --token
databricks bundle deploy --target dev
```

### 3Ô∏è‚É£ Run Initial Pipeline (30 minutes, ~$15)

```bash
# Option A: Via Databricks CLI (recommended for cost control)
databricks jobs run-now --job-id <job-id>

# Option B: Via Python (local development)
python -m pytest tests/unit  # Runs locally, $0
```

### 4Ô∏è‚É£ Deploy Dashboard (5 minutes, ~$5/month)

```bash
cd app
docker build -t nyc311-dashboard .
# Deploy to AWS ECS Fargate (or run locally)
streamlit run streamlit_app.py
```

---

## üéì ML Engineering Best Practices

### ‚úÖ Code Quality
- **Type Hints**: Full static typing with `mypy`
- **Linting**: `ruff` for fast, modern Python linting
- **Formatting**: `black` for consistent code style
- **Testing**: 85%+ coverage with `pytest`

### ‚úÖ Data Engineering
- **Delta Lake**: ACID transactions, time travel, schema evolution
- **Incremental Processing**: Only process new/changed data
- **Data Quality**: Automated expectations with Great Expectations
- **Partitioning**: Optimized by `ingestion_date` for pruning

### ‚úÖ ML Operations
- **Experiment Tracking**: MLflow for all model training
- **Model Registry**: Versioned models with staging/production aliases
- **Hyperparameter Tuning**: Optuna for efficient search
- **Feature Store**: Databricks Feature Store for reusability

### ‚úÖ Observability
- **Pipeline Monitoring**: Success/failure rates, duration metrics
- **Data Quality Alerts**: Automated anomaly detection on data drift
- **Cost Tracking**: Daily spend alerts (AWS CloudWatch + SNS)
- **Model Performance**: Prediction accuracy, MAPE, residuals

---

## üìä Data Pipeline Details

### Bronze Layer (Raw Ingestion)
```python
# Daily incremental load (last 7 days with 1-day overlap)
# Estimated: 50K-100K records/day
# Storage: ~10MB/day compressed (Delta)
# Cost: $0.02/day S3 storage
```

### Silver Layer (Cleaned Data)
```python
# Apply data quality rules
# - Remove duplicates (unique_key deduplication)
# - Validate schema (complaint_type not null)
# - Standardize dates (UTC timezone)
# - Enrich with borough lookup
# Cost: 2 min job runtime = $0.30
```

### Gold Layer (Aggregates)
```python
# Daily aggregations by:
# - complaint_type
# - borough
# - agency
# Features: rolling averages, lag features
# Cost: 1 min job runtime = $0.15
```

---

## ü§ñ ML Model Details

### Forecasting Model (Prophet)
- **Objective**: Predict next 30 days of service requests
- **Features**: Trend, weekly seasonality, holidays
- **Training**: Weekly on 90-day rolling window
- **Hyperparameters**: Tuned via Optuna (20 trials)
- **Metrics**: MAPE, MAE, RMSE
- **Cost**: 10 min training = $1.50/week

### Anomaly Detection
- **Method**: Statistical thresholds from Prophet uncertainty intervals
- **Logic**: Flag if actual > upper_bound (95% confidence)
- **Alerts**: Triggered for 3+ consecutive anomalies
- **Cost**: 1 min inference = $0.15/day

---

## üìà Success Metrics

### Technical KPIs
- **Pipeline SLA**: 99.5% daily completion rate
- **Data Freshness**: <2 hours lag from API to Gold layer
- **Model Accuracy**: MAPE <15% on 7-day forecast
- **False Positive Rate**: <5% on anomaly detection

### Business KPIs
- **Early Detection**: Identify spikes 2-3 days before peak
- **Resource Optimization**: 20% improvement in crew scheduling
- **Cost Efficiency**: Maintain <$140/month operational cost (3 environments)
- **Cost Efficiency (Optimized)**: <$80/month with on-demand QA

---

## üîí Security & Compliance

- **Secrets Management**: AWS Secrets Manager (no hardcoded credentials)
- **IAM Roles**: Least-privilege access for Databricks ‚Üí S3
- **Data Encryption**: At-rest (S3 SSE-S3) and in-transit (TLS 1.2+)
- **Audit Logging**: Delta Lake transaction logs + CloudTrail
- **PII Handling**: No PII in 311 data, butÊû∂ÊûÑ supports masking if needed

---

## üêõ Troubleshooting

### Issue: Job fails with "Out of Memory"
**Solution**: Increase worker node size or optimize data partitioning
```python
# Re-partition Silver table
spark.read.table("silver.requests").repartition(10).write.mode("overwrite").saveAsTable("silver.requests")
```

### Issue: Forecast model poor accuracy
**Solution**: Check data quality, extend training window
```python
# Validate no missing dates in training data
df.groupBy(f.window("created_date", "1 day")).count().orderBy("window")
```

### Issue: Exceeding $100 budget
**Solution**: Review cluster usage, implement auto-termination
```bash
# Check spend
databricks jobs list --output JSON | jq '.jobs[] | select(.settings.max_concurrent_runs > 1)'
```

---

## üéØ Roadmap

### Phase 1: PoC (Current - Week 4)
- ‚úÖ Basic medallion architecture
- ‚úÖ Prophet forecasting
- ‚úÖ Threshold-based anomaly detection
- ‚úÖ Streamlit dashboard

### Phase 2: Production Hardening (Week 5-8)
- ‚¨ú Advanced anomaly detection (Isolation Forest, LSTM)
- ‚¨ú Real-time streaming (Kinesis + Structured Streaming)
- ‚¨ú Automated retraining pipeline
- ‚¨ú A/B testing framework

### Phase 3: Scale & Expand (Week 9-12)
- ‚¨ú Multi-city expansion (Chicago, LA, Boston)
- ‚¨ú Predictive maintenance for city assets
- ‚¨ú Citizen-facing mobile app integration
- ‚¨ú Advanced NLP on complaint descriptions

---

## üë• Contributing

```bash
# Create feature branch
git checkout -b feature/your-feature-name

# Make changes, add tests
pytest tests/

# Submit PR with cost impact analysis
# Include: "Estimated cost change: +$X/month"
```

---

## üìö References

- [NYC 311 Open Data](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9)
- [Databricks Cost Optimization](https://docs.databricks.com/optimizations/cost-optimization.html)
- [Delta Lake Best Practices](https://docs.delta.io/latest/best-practices.html)
- [Prophet Documentation](https://facebook.github.io/prophet/)
- [MLflow Guide](https://mlflow.org/docs/latest/index.html)

---

## üìù License

MIT License - See LICENSE file

---





