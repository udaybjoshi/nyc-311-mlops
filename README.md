# ğŸ“Š NYC 311 Service Request Intelligence Platform (AWS + Databricks Edition)

## ğŸ¯ Project Objective

Build a **real-time, production-grade MLOps pipeline** on AWS and Databricks that:

- Ingests NYC 311 data daily from the API
- Follows a Medallion Architecture: Bronze â†’ Silver â†’ Gold
- Forecasts service request volumes using Prophet
- Flags anomalies based on model thresholds
- Tracks experiments using MLflow (Databricks-native)
- Visualizes insights via Streamlit
- Is containerized and CI/CD-enabled for deployment

---

## ğŸ§± Project Architecture Overview

| Layer               | Technology Used                                         |
|--------------------|----------------------------------------------------------|
| Ingestion          | Python, S3, Databricks Auto Loader                       |
| Transformation     | PySpark, Delta Lake (Bronze â†’ Silver â†’ Gold)             |
| Forecasting        | Prophet, Optuna (on Databricks Jobs)                     |
| Anomaly Detection  | Prophet thresholds, Z-score (PySpark)                    |
| Experiment Tracking| MLflow (Databricks-native)                               |
| Orchestration      | Databricks Workflows or Prefect on AWS                   |
| Dashboard          | Streamlit hosted via ECS or EC2                          |
| CI/CD              | GitHub Actions, Docker, AWS ECS, Databricks REST API     |
| Testing            | Pytest (unit + integration against Delta tables)         |

---

## ğŸ” Medallion Architecture Flow

API â†’ S3 (raw) â†’ Databricks Auto Loader â†’ Bronze Delta
â†’ Silver Delta (cleaned & normalized)
â†’ Gold Delta (aggregated for forecasting & dashboard)



---

## ğŸ”§ Component Directory Structure

nyc-311-mlops/
â”œâ”€â”€ ingestion/ # API to S3, Auto Loader setup
â”œâ”€â”€ transformation/ # Bronze â†’ Silver â†’ Gold Delta jobs
â”œâ”€â”€ ml/ # Forecasting + anomaly detection
â”œâ”€â”€ orchestration/ # Databricks Workflows or Prefect flows
â”œâ”€â”€ dashboards/ # Streamlit app
â”œâ”€â”€ tests/ # Unit and integration tests
â”œâ”€â”€ docker-compose.yml # Local testing
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md # This file
â””â”€â”€ .github/workflows/ci.yml # CI/CD pipeline



---

## ğŸš€ How to Run the Pipeline (Dev Mode)

### 1. Ingest Data to S3
python ingestion/fetch_api_data.py

### 2. Load Raw Data to Databricks (Bronze)
Handled automatically by Databricks Auto Loader.

### 3. Transform: Bronze â†’ Silver â†’ Gold
Run notebook jobs or Databricks Workflows via UI or API.

### 4. Forecast & Flag Anomalies
python ml/generate_forecasts.py

### 5. Track ML Experiments
Logged automatically to Databricks MLflow.

### 6. View Dashboard
streamlit run dashboards/app.py

## ğŸ“ˆ Deliverables to Showcase

| Deliverable                      | Value                                               |
| -------------------------------- | --------------------------------------------------- |
| ğŸ“Š Forecast + anomaly dashboard  | Demonstrates real-time insights for stakeholders    |
| ğŸ§ª MLflow + Optuna tuning logs   | Showcases experimentation and model reproducibility |
| ğŸ§Š Delta Lake Medallion pipeline | Scalable, modular data engineering architecture     |
| âš™ï¸ Databricks Workflows          | Reliable automation of daily flows                  |
| ğŸ³ Docker + CI/CD pipeline       | Proves deployment readiness and DevOps maturity     |
| âœ… Tests (unit + integration)     | Ensures production-grade quality assurance          |

## ğŸ” Deployment
- Streamlit App: Deployed via Docker to AWS ECS Fargate or EC2
- Forecasting Pipeline: Scheduled via Databricks Workflows
- CI/CD: GitHub Actions â†’ Docker Build â†’ Deploy to ECS & Databricks Jobs via API

## ğŸ§ª Testing
Run unit and integration tests locally:
pytest tests/

To test against Delta tables in Databricks:
- Use the Databricks SQL Connector or pytest-databricks-connect
