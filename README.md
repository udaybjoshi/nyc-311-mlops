# ğŸ—½ NYC 311 Service Request Intelligence Platform - PoC

A **cost-optimized, production-grade data pipeline** for NYC 311 service request analysis using Databricks, AWS, and the Medallion Architecture. Built to stay **under $100** while demonstrating enterprise ML engineering practices.

---

## ğŸ“Š Project Overview

This platform ingests NYC 311 service request data, processes it through Bronze â†’ Silver â†’ Gold layers, applies ML-based forecasting and anomaly detection, and delivers insights via an interactive dashboard.

### ğŸ¯ Business Value
- **Proactive Operations**: Detect anomalous spikes in service requests before they become critical
- **Resource Optimization**: Forecast demand patterns for better workforce allocation
- **Cost Efficiency**: 95% cost reduction vs. traditional 24/7 cluster approaches
- **Transparency**: Full ML lineage, auditability, and explainable insights

---

## ğŸ’° Cost Optimization Strategy (Target: <$100)

| Component | Strategy | Est. Cost |
|-----------|----------|-----------|
| **Databricks Compute** | Job clusters (auto-terminate) + Community Edition for dev | $30-40 |
| **AWS S3 Storage** | Lifecycle policies, compressed Delta tables | $5-10 |
| **Data Volume** | Sample last 90 days only (vs. 10+ years full dataset) | $0 |
| **API Calls** | Cached responses, incremental loads | $0 |
| **Development** | Local Spark for testing, CI/CD optimizations | $5-10 |
| **Monitoring** | Built-in Databricks metrics (no external tools) | $0 |
| **Total Estimate** | | **$40-60** |

### ğŸ”§ Cost Control Measures
- âœ… **Job Clusters**: Spin up only when needed, terminate after 5 min idle
- âœ… **Spot Instances**: 70% discount on AWS spot for batch workloads
- âœ… **Data Sampling**: Process 90-day rolling window (not full 10+ year dataset)
- âœ… **Incremental Processing**: Delta Lake change data capture
- âœ… **Auto-scaling**: Min 1 worker, max 3 workers
- âœ… **Scheduled Jobs**: Run daily during off-peak hours (3 AM ET)
- âœ… **Local Development**: Unit tests run on local Spark (no cluster costs)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NYC Open Data  â”‚
â”‚   311 API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Daily Ingestion (Scheduled Job)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BRONZE LAYER (Raw Data)          â”‚
â”‚  â€¢ Raw JSON from API                     â”‚
â”‚  â€¢ Append-only Delta tables              â”‚
â”‚  â€¢ Partition by ingestion_date           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Data Quality + Deduplication
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SILVER LAYER (Cleaned Data)         â”‚
â”‚  â€¢ Standardized schema                   â”‚
â”‚  â€¢ Type casting + validation             â”‚
â”‚  â€¢ Business rules applied                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Aggregation + Feature Engineering
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    GOLD LAYER (Analytics-Ready Data)     â”‚
â”‚  â€¢ Daily aggregates by complaint_type    â”‚
â”‚  â€¢ Time series features                  â”‚
â”‚  â€¢ Ready for ML + BI tools               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                 â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Prophet â”‚    â”‚   Anomaly    â”‚  â”‚Streamlit â”‚
    â”‚Forecast â”‚    â”‚  Detection   â”‚  â”‚Dashboard â”‚
    â”‚ Model   â”‚    â”‚  (Threshold) â”‚  â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    MLflow    â”‚
          â”‚  Experiment  â”‚
          â”‚   Tracking   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
nyc-311-platform/
â”‚
â”œâ”€â”€ .databricks/                 # Databricks configuration
â”‚   â””â”€â”€ bundle.yml              # Databricks Asset Bundle config
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml              # CI/CD pipeline
â”‚       â””â”€â”€ deploy.yml          # Deployment automation
â”‚
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ dev.yaml                # Development environment config
â”‚   â”œâ”€â”€ prod.yaml               # Production environment config
â”‚   â””â”€â”€ cluster_config.yaml     # Cost-optimized cluster specs
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_setup_and_exploration.py
â”‚   â”œâ”€â”€ 01_bronze_ingestion.py
â”‚   â”œâ”€â”€ 02_silver_transformation.py
â”‚   â”œâ”€â”€ 03_gold_aggregation.py
â”‚   â”œâ”€â”€ 04_ml_forecasting.py
â”‚   â”œâ”€â”€ 05_anomaly_detection.py
â”‚   â””â”€â”€ 99_monitoring_dashboard.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ nyc311/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ api_client.py
â”‚   â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚   â”‚   â”‚   â””â”€â”€ silver_to_gold.py
â”‚   â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ forecasting.py
â”‚   â”‚   â”‚   â””â”€â”€ anomaly_detection.py
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”‚   â””â”€â”€ delta_helpers.py
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ metrics.py
â”‚   â””â”€â”€ setup.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_api_client.py
â”‚   â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”‚   â””â”€â”€ test_ml_models.py
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_pipeline_e2e.py
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_app.py        # Dashboard application
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf             # AWS S3 + IAM setup
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ setup_workspace.sh
â”‚       â””â”€â”€ cost_monitor.sh
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ cost_analysis.md
â”‚   â””â”€â”€ runbook.md
â”‚
â”œâ”€â”€ .env.example                # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ databricks.yml              # Databricks CLI config
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ requirements-dev.txt        # Development dependencies
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start (Under $100 Budget)

### Prerequisites
- AWS Account (free tier eligible)
- Databricks Account (Community Edition or trial)
- Python 3.9+
- Databricks CLI

### 1ï¸âƒ£ Initial Setup (5 minutes, $0)

```bash
# Clone repository
git clone <your-repo-url>
cd nyc-311-platform

# Install dependencies locally (for development)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements-dev.txt

# Configure environment
cp .env.example .env
# Edit .env with your credentials
```

### 2ï¸âƒ£ Infrastructure Setup (10 minutes, ~$5)

```bash
# Setup AWS resources (S3 buckets with lifecycle policies)
cd infrastructure/terraform
terraform init
terraform plan
terraform apply -var="environment=dev"

# Configure Databricks workspace
databricks configure --token
databricks bundle deploy --target dev
```

### 3ï¸âƒ£ Run Initial Pipeline (30 minutes, ~$15)

```bash
# Option A: Via Databricks CLI (recommended for cost control)
databricks jobs run-now --job-id <job-id>

# Option B: Via Python (local development)
python -m pytest tests/unit  # Runs locally, $0
```

### 4ï¸âƒ£ Deploy Dashboard (5 minutes, ~$5/month)

```bash
cd app
docker build -t nyc311-dashboard .
# Deploy to AWS ECS Fargate (or run locally)
streamlit run streamlit_app.py
```

---

## ğŸ“ ML Engineering Best Practices

### âœ… Code Quality
- **Type Hints**: Full static typing with `mypy`
- **Linting**: `ruff` for fast, modern Python linting
- **Formatting**: `black` for consistent code style
- **Testing**: 85%+ coverage with `pytest`

### âœ… Data Engineering
- **Delta Lake**: ACID transactions, time travel, schema evolution
- **Incremental Processing**: Only process new/changed data
- **Data Quality**: Automated expectations with Great Expectations
- **Partitioning**: Optimized by `ingestion_date` for pruning

### âœ… ML Operations
- **Experiment Tracking**: MLflow for all model training
- **Model Registry**: Versioned models with staging/production aliases
- **Hyperparameter Tuning**: Optuna for efficient search
- **Feature Store**: Databricks Feature Store for reusability

### âœ… Observability
- **Pipeline Monitoring**: Success/failure rates, duration metrics
- **Data Quality Alerts**: Automated anomaly detection on data drift
- **Cost Tracking**: Daily spend alerts (AWS CloudWatch + SNS)
- **Model Performance**: Prediction accuracy, MAPE, residuals

---

## ğŸ“Š Data Pipeline Details

### Bronze Layer (Raw Ingestion)
```python
# Daily incremental load (last 7 days with 1-day overlap)
# Estimated: 50K-100K records/day
# Storage: ~10MB/day compressed (Delta)
# Cost: $0.02/day S3 storage
```

### Silver Layer (Cleaned Data)
```python
# Apply data quality rules
# - Remove duplicates (unique_key deduplication)
# - Validate schema (complaint_type not null)
# - Standardize dates (UTC timezone)
# - Enrich with borough lookup
# Cost: 2 min job runtime = $0.30
```

### Gold Layer (Aggregates)
```python
# Daily aggregations by:
# - complaint_type
# - borough
# - agency
# Features: rolling averages, lag features
# Cost: 1 min job runtime = $0.15
```

---

## ğŸ¤– ML Model Details

### Forecasting Model (Prophet)
- **Objective**: Predict next 30 days of service requests
- **Features**: Trend, weekly seasonality, holidays
- **Training**: Weekly on 90-day rolling window
- **Hyperparameters**: Tuned via Optuna (20 trials)
- **Metrics**: MAPE, MAE, RMSE
- **Cost**: 10 min training = $1.50/week

### Anomaly Detection
- **Method**: Statistical thresholds from Prophet uncertainty intervals
- **Logic**: Flag if actual > upper_bound (95% confidence)
- **Alerts**: Triggered for 3+ consecutive anomalies
- **Cost**: 1 min inference = $0.15/day

---

## ğŸ“ˆ Success Metrics

### Technical KPIs
- **Pipeline SLA**: 99.5% daily completion rate
- **Data Freshness**: <2 hours lag from API to Gold layer
- **Model Accuracy**: MAPE <15% on 7-day forecast
- **False Positive Rate**: <5% on anomaly detection

### Business KPIs
- **Early Detection**: Identify spikes 2-3 days before peak
- **Resource Optimization**: 20% improvement in crew scheduling
- **Cost Efficiency**: Maintain <$140/month operational cost (3 environments)
- **Cost Efficiency (Optimized)**: <$80/month with on-demand QA

---

## ğŸ”’ Security & Compliance

- **Secrets Management**: AWS Secrets Manager (no hardcoded credentials)
- **IAM Roles**: Least-privilege access for Databricks â†’ S3
- **Data Encryption**: At-rest (S3 SSE-S3) and in-transit (TLS 1.2+)
- **Audit Logging**: Delta Lake transaction logs + CloudTrail
- **PII Handling**: No PII in 311 data, butæ¶æ„ supports masking if needed

---

## ğŸ› Troubleshooting

### Issue: Job fails with "Out of Memory"
**Solution**: Increase worker node size or optimize data partitioning
```python
# Re-partition Silver table
spark.read.table("silver.requests").repartition(10).write.mode("overwrite").saveAsTable("silver.requests")
```

### Issue: Forecast model poor accuracy
**Solution**: Check data quality, extend training window
```python
# Validate no missing dates in training data
df.groupBy(f.window("created_date", "1 day")).count().orderBy("window")
```

### Issue: Exceeding $100 budget
**Solution**: Review cluster usage, implement auto-termination
```bash
# Check spend
databricks jobs list --output JSON | jq '.jobs[] | select(.settings.max_concurrent_runs > 1)'
```

---

## ğŸ¯ Roadmap

### Phase 1: PoC (Current - Week 4)
- âœ… Basic medallion architecture
- âœ… Prophet forecasting
- âœ… Threshold-based anomaly detection
- âœ… Streamlit dashboard

### Phase 2: Production Hardening (Week 5-8)
- â¬œ Advanced anomaly detection (Isolation Forest, LSTM)
- â¬œ Real-time streaming (Kinesis + Structured Streaming)
- â¬œ Automated retraining pipeline
- â¬œ A/B testing framework

### Phase 3: Scale & Expand (Week 9-12)
- â¬œ Multi-city expansion (Chicago, LA, Boston)
- â¬œ Predictive maintenance for city assets
- â¬œ Citizen-facing mobile app integration
- â¬œ Advanced NLP on complaint descriptions

---

## ğŸ‘¥ Contributing

```bash
# Create feature branch
git checkout -b feature/your-feature-name

# Make changes, add tests
pytest tests/

# Submit PR with cost impact analysis
# Include: "Estimated cost change: +$X/month"
```

---

## ğŸ“š References

- [NYC 311 Open Data](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9)
- [Databricks Cost Optimization](https://docs.databricks.com/optimizations/cost-optimization.html)
- [Delta Lake Best Practices](https://docs.delta.io/latest/best-practices.html)
- [Prophet Documentation](https://facebook.github.io/prophet/)
- [MLflow Guide](https://mlflow.org/docs/latest/index.html)

---

## ğŸ“ License

MIT License - See LICENSE file

---





